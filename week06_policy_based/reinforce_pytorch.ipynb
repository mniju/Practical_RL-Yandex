{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REINFORCE in pytorch\n",
    "\n",
    "Just like we did before for q-learning, this time we'll design a pytorch network to learn `CartPole-v0` via policy gradient (REINFORCE).\n",
    "\n",
    "Most of the code in this notebook is taken from approximate qlearning, so you'll find it more or less familiar and even simpler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: DISPLAY=: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'bash' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "# # in google colab uncomment this\n",
    "\n",
    "# import os\n",
    "\n",
    "# os.system('apt-get install -y xvfb')\n",
    "# os.system('wget https://raw.githubusercontent.com/yandexdataschool/Practical_DL/fall18/xvfb -O ../xvfb')\n",
    "# os.system('apt-get install -y python-opengl ffmpeg')\n",
    "# os.system('pip install pyglet==1.2.4')\n",
    "\n",
    "# os.system('python -m pip install -U pygame --user')\n",
    "\n",
    "# print('setup complete')\n",
    "\n",
    "# XVFB will be launched if you run on a server\n",
    "import os\n",
    "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
    "    !bash ../xvfb start\n",
    "    %env DISPLAY = : 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4,)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "env = gym.make(\"CartPole-v0\").env\n",
    "env.reset()\n",
    "\n",
    "n_actions = env.action_space.n\n",
    "state_dim = env.observation_space.shape\n",
    "print(state_dim)\n",
    "\n",
    "plt.imshow(env.render(\"rgb_array\"))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the network for REINFORCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For REINFORCE algorithm, we'll need a model that predicts action probabilities given states. Let's define such a model below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a simple neural network that predicts policy logits. \n",
    "# Keep it simple: CartPole isn't worth deep architectures.\n",
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# those who have a GPU but feel unfair to use it can uncomment:\n",
    "\n",
    "model = nn.Sequential(\n",
    "            nn.Linear(state_dim[0],200),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(200,n_actions),\n",
    "  #< YOUR CODE HERE: define a neural network that predicts policy logits >\n",
    ")\n",
    "#model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_probs(states):\n",
    "    \"\"\" \n",
    "    Predict action probabilities given states.\n",
    "    :param states: numpy array of shape [batch, state_shape]\n",
    "    :returns: numpy array of shape [batch, n_actions]\n",
    "    \"\"\"\n",
    "    # convert states, compute logits, use softmax to get probability\n",
    "    #<your code here >\n",
    "    states = torch.tensor(states, dtype=torch.float) \n",
    "    logits = model(states)\n",
    "    probs = nn.functional.softmax(logits, 1)#Softmax is used instaed of takingt he Q values and calculating the maximum value\n",
    "    probs = probs.detach().numpy()\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_states = np.array([env.reset() for _ in range(5)])\n",
    "test_probas = predict_probs(test_states)\n",
    "assert isinstance(\n",
    "    test_probas, np.ndarray), \"you must return np array and not %s\" % type(test_probas)\n",
    "assert tuple(test_probas.shape) == (\n",
    "    test_states.shape[0], env.action_space.n), \"wrong output shape: %s\" % np.shape(test_probas)\n",
    "assert np.allclose(np.sum(test_probas, axis=1),\n",
    "                   1), \"probabilities do not sum to 1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Play the game\n",
    "\n",
    "We can now use our newly built agent to play the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_session(t_max=1000):\n",
    "    \"\"\" \n",
    "    play a full session with REINFORCE agent and train at the session end.\n",
    "    returns sequences of states, actions andrewards\n",
    "    \"\"\"\n",
    "    # arrays to record session\n",
    "    states, actions, rewards = [], [], []\n",
    "    s = env.reset()\n",
    "\n",
    "    for t in range(t_max):\n",
    "        # action probabilities array aka pi(a|s)\n",
    "        action_probs = predict_probs(np.array([s]))[0] \n",
    "\n",
    "        # Sample action with given probabilities.\n",
    "        #a = np.argmax(action_probs)\n",
    "        a = np.random.choice(n_actions,1,p=action_probs)[0]\n",
    "        new_s, r, done, info = env.step(a)\n",
    "\n",
    "        # record session history to train later\n",
    "        states.append(s)\n",
    "        actions.append(a)\n",
    "        rewards.append(r)\n",
    "\n",
    "        s = new_s\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    return states, actions, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test it\n",
    "states, actions, rewards = generate_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing cumulative rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cumulative_rewards(rewards,  # rewards at each step\n",
    "                           gamma=0.99  # discount for reward\n",
    "                           ):\n",
    "    \"\"\"\n",
    "    take a list of immediate rewards r(s,a) for the whole session \n",
    "    compute cumulative returns (a.k.a. G(s,a) in Sutton '16)\n",
    "    G_t = r_t + gamma*r_{t+1} + gamma^2*r_{t+2} + ...\n",
    "\n",
    "    The simple way to compute cumulative rewards is to iterate from last to first time tick\n",
    "    and compute G_t = r_t + gamma*G_{t+1} recurrently\n",
    "\n",
    "    You must return an array/list of cumulative rewards with as many elements as in the initial rewards.\n",
    "    \"\"\"\n",
    "    #your code here >\n",
    "    G_t = [rewards[-1]]# Last Reward\n",
    "    for i in range(len(rewards)-2,-1,-1):\n",
    "        G_t.append(rewards[i] +gamma*G_t[-1])#Append Returns\n",
    "    return G_t[::-1]#Reverse the listback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "looks good!\n"
     ]
    }
   ],
   "source": [
    "get_cumulative_rewards(rewards)\n",
    "assert len(get_cumulative_rewards(list(range(100)))) == 100\n",
    "assert np.allclose(get_cumulative_rewards([0, 0, 1, 0, 0, 1, 0], gamma=0.9), [\n",
    "                   1.40049, 1.5561, 1.729, 0.81, 0.9, 1.0, 0.0])\n",
    "assert np.allclose(get_cumulative_rewards(\n",
    "    [0, 0, 1, -2, 3, -4, 0], gamma=0.5), [0.0625, 0.125, 0.25, -1.5, 1.0, -4.0, 0.0])\n",
    "assert np.allclose(get_cumulative_rewards(\n",
    "    [0, 0, 1, 2, 3, 4, 0], gamma=0), [0, 0, 1, 2, 3, 4, 0])\n",
    "print(\"looks good!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss function and updates\n",
    "\n",
    "We now need to define objective and update over policy gradient.\n",
    "\n",
    "Our objective function is\n",
    "\n",
    "$$ J \\approx  { 1 \\over N } \\sum  _{s_i,a_i} \\pi_\\theta (a_i | s_i) \\cdot G(s_i,a_i) $$\n",
    "\n",
    "\n",
    "Following the REINFORCE algorithm, we can define our objective as follows: \n",
    "\n",
    "$$ \\hat J \\approx { 1 \\over N } \\sum  _{s_i,a_i} log \\pi_\\theta (a_i | s_i) \\cdot G(s_i,a_i) $$\n",
    "\n",
    "When you compute gradient of that function over network weights $ \\theta $, it will become exactly the policy gradient.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_one_hot(y_tensor, ndims):\n",
    "    \"\"\" helper: take an integer vector and convert it to 1-hot matrix. \"\"\"\n",
    "    y_tensor = y_tensor.type(torch.LongTensor).view(-1, 1)\n",
    "    y_one_hot = torch.zeros(\n",
    "        y_tensor.size()[0], ndims).scatter_(1, y_tensor, 1)\n",
    "    return y_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code: define optimizers\n",
    "optimizer = torch.optim.Adam(model.parameters(), 1e-2)\n",
    "\n",
    "\n",
    "def train_on_session(states, actions, rewards, gamma=0.99, entropy_coef=1e-2):\n",
    "    \"\"\"\n",
    "    Takes a sequence of states, actions and rewards produced by generate_session.\n",
    "    Updates agent's weights by following the policy gradient above.\n",
    "    Please use Adam optimizer with default parameters.\n",
    "    \"\"\"\n",
    "\n",
    "    # cast everything into torch tensors\n",
    "    states = torch.tensor(states, dtype=torch.float32)\n",
    "    actions = torch.tensor(actions, dtype=torch.int32)\n",
    "    cumulative_returns = np.array(get_cumulative_rewards(rewards, gamma))\n",
    "    cumulative_returns = torch.tensor(cumulative_returns, dtype=torch.float32)\n",
    "\n",
    "    # predict logits, probas and log-probas using an agent.\n",
    "    logits = model(states)\n",
    "    probs = nn.functional.softmax(logits, -1)\n",
    "    log_probs = nn.functional.log_softmax(logits, -1)\n",
    "\n",
    "    assert all(isinstance(v, torch.Tensor) for v in [logits, probs, log_probs]), \\\n",
    "        \"please use compute using torch tensors and don't use predict_probs function\"\n",
    "\n",
    "    # select log-probabilities for chosen actions, log pi(a_i|s_i)\n",
    "    log_probs_for_actions = torch.sum(\n",
    "        log_probs * to_one_hot(actions, env.action_space.n), dim=1)\n",
    "   \n",
    "    \n",
    "    # Compute loss here. Don't forgen entropy regularization with `entropy_coef` \n",
    "    #1. define Objective Function\n",
    "    J = torch.mean(log_probs_for_actions *cumulative_returns)\n",
    "    entropy = -(probs*log_probs).sum()\n",
    "    loss = -(J-entropy_coef * entropy) \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    # Gradient descent step\n",
    "    #< your code >\n",
    "    optimizer.step()\n",
    "    \n",
    "\n",
    "    # technical: return session rewards to print them later\n",
    "    return np.sum(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The actual training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean reward:35.820\n",
      "mean reward:15.510\n",
      "mean reward:31.570\n",
      "mean reward:450.980\n",
      "mean reward:548.590\n",
      "You Win!\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    rewards = [train_on_session(*generate_session())\n",
    "               for _ in range(100)]  # generate new sessions\n",
    "    print(\"mean reward:%.3f\" % (np.mean(rewards)))\n",
    "    if np.mean(rewards) > 500:\n",
    "        print(\"You Win!\")  # but you can train even further\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# record sessions\n",
    "import gym.wrappers\n",
    "env = gym.wrappers.Monitor(gym.make(\"CartPole-v0\"),\n",
    "                           directory=\"videos\", force=True)\n",
    "sessions = [generate_session() for _ in range(100)]\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show video\n",
    "from IPython.display import HTML\n",
    "import os\n",
    "\n",
    "video_names = list(\n",
    "    filter(lambda s: s.endswith(\".mp4\"), os.listdir(\"./videos/\")))\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(\"./videos/\"+video_names[-1]))  # this may or may not be the _last_ video. Try other indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
